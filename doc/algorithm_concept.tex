\chapter{The algorithm}

The training dataset consists of approximately 560h of subtitled audio. From this, we extract MFCC features and approximate timestamps for each single word. For each word $w$ we aim to define a $\mathcal{C}^1$ function \(D_w\colon \R \to \lbrack 0, \infty)\) such that $D_w(t)$ is large iff the audio around time $t$ sounds like the word $w$. For a given transcript $\mathcal{T}$ and audio $A$, we can find the function $f$ introduced in chapter \ref{chap:intro} as the solution to the minimization problem
\begin{align*}
	&\min_x -\sum_{j=1}^l D_{w_j}(x_j)\\
	&\hspace{0.5em}\text{s.t. }x_1\leq x_2\leq \dotsc\leq x_l
\end{align*}

\section{Defining the distance metric}

The functions \(D_w\colon \R \to \lbrack 0, \infty)\) can be seen as equivalent to a metric for the distance between the sound of the word $w$ and the audio $A$ at $t$. They need to be constructed in a way such that
\begin{enumerate}
	\item $D_w$ is guaranteed to be continuously differentiable w.r.t. $t$ for every word $w$.
	\item $D_w$ corresponds to the probability, that the word $w$ is said as time $t$ in $A$. More specifically, the function mapping said probability $p$ to $D_w(t)$ should be monotonically increasing.
\end{enumerate}

One important motivating factor behind the construction of this optimization objective is, that the dataset does not contain exact time points for each word. Instead, for any given word $w$, we assume that the difference between the time point in the dataset $t_\text{data}$ and the actual time point in the audio at which $w$ is being said $t_\text{true}$ follows a normal distribution with mean $0$ and constant standard deviation $\sigma$.

Divide the audio into intervals of equal size. Label each interval $I_i$ with the probability of it containing $w$. This can be done by a variety of possible machine learning methods, our specific approach is discussed in section \ref{interval_word_prob}. These probabilities $p_i$ can be used to infer an observed probability distribution of where in the audio the machine learning model expects $w$ to be located. More precisely, because the same word might appear multiple times, we assume the $p_i$ to be generated by the sum of multiple normal distributions with different means (time points) and constant standard deviation $\sigma$. We also assume, that the specific instance $w$ of the word in question can be attributed to exactly one of these distributions with mean $t$.\\
In order to find the optimal time point $t_\text{true}$ we can use the observation, that two normal distributions with the same standard deviation have the same mean $t=t_\text{true}$ exactly if it holds for the probability density functions $\rho_{t,\sigma}$ and $\rho_{t_\text{true},\sigma}$ that
\[t = \arg\max \int_\R \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x\]
Hence, we can use the maximum value on the right hand side as a score for how similar the audio around time $t$ the model considers the word $w$ to be. We can also show that this does not require knowledge of $t_\text{true}$ in practice:
\begin{align*}
	\int_\R \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x &= \sum_{i} \int_{I_i} \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x\\
			&= \sum_i \rho_{t,\sigma}(\xi_i) \underbrace{\int_{I_i} \rho_{t_\text{true},\sigma}(x)\text{d}x}_{\approx p_i}
\end{align*}
by the mean value theorem for a sufficient $\xi_i\in I_i$ (because $\rho_{t_\text{true},\sigma}(x) > 0$ for all $x$). Equality still approximately holds if we set $\xi_i$ to be the center point of $I_i$ for sufficiently small intervals.
Hence, we can define $D_w$ through
\[
	D_w(t) = \sum_i p_i \rho_{t,\sigma}(t_i)
\]
for each center point $t_i$ of $I_i$. This construction satisfies the smoothness required for optimization.

\section{Computing probabilities for each interval}
\label{interval_word_prob}

\textbf{by Paul Warkentin} \\

Given an audio file, we divide its MFCC features into $n$ intervals $I_i$ of equal lengths $l$. If the rest part of the data $I_{n+1}$ is shorter than the length $l$, it is discarded. The result of this section will be a matrix $p$ of shape $\lbrack l, c \rbrack$, where $c$ is the fixed number of words we selected for evaluation.

Since we interpolated the timing of each word in chapter \ref{data_preparation}, we can iterate for all talks through each word $w_j$ with its estimated timing $t_j$. Calculate the cumulative distribution function $\Phi_{t_j, 0.8}(x)$ with a mean value of $t_j$ and a standard deviation of 0.8. \\
The current word $w_j$ lies in the interval with index $k = \lceil \tfrac{t_j}{l} \rceil$. For the interval $I_k$ and its neighbors $\{I_{k-3}, I_{k-2}, I_{k-1}, I_{k+1}, I_{k+2}\}$, do the cumulative mapping
\[p_{j, s} \mapsto p_{j, s} + \Phi_{t_j, 0.8}((s - 1) l) - \Phi_{t_j, 0.8}(s l),\]
for $s = k-3, k-2, \dots, k+2$. \\

Hence, we got the features and its labels for the supervised learning model described in the section below. \\
The MFCC features for an audio file are reshaped into a matrix of shape $\lbrack l, m \rbrack$, where $m$ is the number of MFCC samples for an interval of length $l$.

\section{Defining the models}

\textbf{by Paul Warkentin} \\
