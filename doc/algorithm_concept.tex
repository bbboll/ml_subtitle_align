\chapter{The algorithm}

The training dataset consists of approximately 560h of subtitled audio. From this, we extract MFCC features and approximate timestamps for each single word. For each word $w$ we aim to define a $\mathcal{C}^1$ function \(D_w\colon \R \to \lbrack 0, \infty)\) such that $D_w(t)$ is large iff the audio around time $t$ sounds like the word $w$. For a given transcript $\mathcal{T}$ and audio $A$, we can find the function $f$ introduced in chapter \ref{chap:intro} as the solution to the minimization problem
\begin{align*}
	&\min_x -\sum_{j=1}^l D_{w_j}(x_j)\\
	&\hspace{0.5em}\text{s.t. }x_1\leq x_2\leq \dotsc\leq x_l
\end{align*}

\section{Defining the distance metric}

The functions \(D_w\colon \R \to \lbrack 0, \infty)\) can be seen as equivalent to a metric for the distance between the sound of the word $w$ and the audio $A$ at $t$. They need to be constructed in a way such that
\begin{enumerate}
	\item $D_w$ is guaranteed to be continuously differentiable w.r.t. $t$ for every word $w$.
	\item $D_w$ corresponds to the probability, that the word $w$ is said as time $t$ in $A$. More specifically, the function mapping said probability $p$ to $D_w(t)$ should be monotonically increasing.
\end{enumerate}

One important motivating factor behind the construction of this optimization objective is, that the dataset does not contain exact time points for each word. Instead, for any given word $w$, we assume that the difference between the time point in the dataset $t_\text{data}$ and the actual time point in the audio at which $w$ is being said $t_\text{true}$ follows a normal distribution with mean $0$ and constant standard deviation $\sigma$.

Divide the audio into intervals of equal size. Label each interval $I_i$ with the probability of it containing $w$. This can be done by a variety of possible machine learning methods, our specific approach is discussed in section \ref{interval_word_prob}. These probabilities $p_i$ can be used to infer an observed probability distribution of where in the audio the machine learning model expects $w$ to be located. More precisely, because the same word might appear multiple times, we assume the $p_i$ to be generated by the sum of multiple normal distributions with different means (time points) and constant standard deviation $\sigma$. We also assume, that the specific instance $w$ of the word in question can be attributed to exactly one of these distributions with mean $t$.\\
In order to find the optimal time point $t_\text{true}$ we can use the observation, that two normal distributions with the same standard deviation have the same mean $t=t_\text{true}$ exactly if it holds for the probability density functions $\rho_{t,\sigma}$ and $\rho_{t_\text{true},\sigma}$ that
\[t = \arg\max \int_\R \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x\]
Hence, we can use the maximum value on the right hand side as a score for how similar the audio around time $t$ the model considers the word $w$ to be. We can also show that this does not require knowledge of $t_\text{true}$ in practice:
\begin{align*}
	\int_\R \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x &= \sum_{i} \int_{I_i} \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x\\
			&= \sum_i \rho_{t,\sigma}(\xi_i) \underbrace{\int_{I_i} \rho_{t_\text{true},\sigma}(x)\text{d}x}_{\approx p_i}
\end{align*}
by the mean value theorem for a sufficient $\xi_i\in I_i$ (because $\rho_{t_\text{true},\sigma}(x) > 0$ for all $x$). Equality still approximately holds if we set $\xi_i$ to be the center point of $I_i$ for sufficiently small intervals. 
Hence, we can define $D_w$ through
\[
	D_w(t) = \sum_i p_i \rho_{t,\sigma}(t_i)
\]
for each center point $t_i$ of $I_i$. This construction satisfies the smoothness required for optimization.

\section{Computing probabilities for each interval}
\label{interval_word_prob}

\textbf{by Bastian Boll} \\
















