\chapter{The algorithm}

\textbf{by Bastian Boll} \\

The training dataset consists of approximately 560h of subtitled audio. From this, we extract MFCC features and approximate timestamps for each single word. For each word $w$ we aim to define a $\mathcal{C}^1$ function \(D_w\colon \R \to \lbrack 0, \infty)\) such that $D_w(t)$ is large iff the audio around time $t$ sounds like the word $w$. For a given transcript $\mathcal{T}$ and audio $A$, we can find the function $f$ introduced in chapter \ref{chap:intro} as the solution to the minimization problem
\begin{align*}
	&\min_x -\sum_{j=1}^l D_{w_j}(x_j)\\
	&\hspace{0.5em}\text{s.t. }x_1\leq x_2\leq \dotsc\leq x_l
\end{align*}

\section{Defining the distance metric}

\textbf{by Bastian Boll} \\

The functions \(D_w\colon \R \to \lbrack 0, \infty)\) can be seen as equivalent to a metric for the distance between the sound of the word $w$ and the audio $A$ at $t$. They need to be constructed in a way such that
\begin{enumerate}
	\item $D_w$ is guaranteed to be continuously differentiable w.r.t. $t$ for every word $w$.
	\item $D_w$ corresponds to the probability, that the word $w$ is said as time $t$ in $A$. More specifically, the function mapping said probability $p$ to $D_w(t)$ should be monotonically increasing.
\end{enumerate}

One important motivating factor behind the construction of this optimization objective is, that the dataset does not contain exact time points for each word. Instead, for any given word $w$, we assume that the difference between the time point in the dataset $t_\text{data}$ and the actual time point in the audio at which $w$ is being said $t_\text{true}$ follows a normal distribution with mean $0$ and constant standard deviation $\sigma$.

Divide the audio into intervals of equal size. Label each interval $I_i$ with the probability of it containing $w$. This can be done by a variety of possible machine learning methods, our specific approach is discussed in section \ref{interval_word_prob}. These probabilities $p_i$ can be used to infer an observed probability distribution of where in the audio the machine learning model expects $w$ to be located. More precisely, because the same word might appear multiple times, we assume the $p_i$ to be generated by the sum of multiple normal distributions with different means (time points) and constant standard deviation $\sigma$. We also assume, that the specific instance $w$ of the word in question can be attributed to exactly one of these distributions with mean $t$.\\
In order to find the optimal time point $t_\text{true}$ we can use the observation, that two normal distributions with the same standard deviation have the same mean $t=t_\text{true}$ exactly if it holds for the probability density functions $\rho_{t,\sigma}$ and $\rho_{t_\text{true},\sigma}$ that
\[t = \arg\max \int_\R \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x\]
Hence, we can use the maximum value on the right hand side as a score for how similar the audio around time $t$ the model considers the word $w$ to be. This can be transformed into an objective in terms of the probabilities $p_i$ computed by the machine learning model
\begin{align*}
	\int_\R \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x &= \sum_{i} \int_{I_i} \rho_{t_\text{true},\sigma}(x)\rho_{t,\sigma}(x)\text{d}x\\
			&= \sum_i \rho_{t,\sigma}(\xi_i) \underbrace{\int_{I_i} \rho_{t_\text{true},\sigma}(x)\text{d}x}_{\approx p_i}
\end{align*}
by the mean value theorem for a sufficient $\xi_i\in I_i$ (because $\rho_{t_\text{true},\sigma}(x) > 0$ for all $x$). Equality still approximately holds if we set $\xi_i$ to be the center point of $I_i$ for sufficiently small intervals.
Hence, we can define $D_w$ through
\[
	D_w(t) = \sum_i p_i \rho_{t,\sigma}(t_i)
\]
for each center point $t_i$ of $I_i$. This construction satisfies the smoothness required for optimization. It also considers the inherent imprecision of word time points in the dataset and provides a means of leveraging information computed by the machine learning model without requiring perfect prediction accuracy.

\section{Computing probabilities for each interval}
\label{interval_word_prob}

\textbf{by Paul Warkentin} \\

Given an audio file, we divide its MFCC features into $n$ intervals $I_i$ of equal lengths $l$. If the rest part of the data $I_{n+1}$ is shorter than the length $l$, it is discarded. The result of this section will be a matrix $p$ of shape $\lbrack l, c \rbrack$, where $c$ is the fixed number of words we selected for evaluation.

Since we interpolated the timing of each word in chapter \ref{data_preparation}, we can iterate for all talks through each word $w_j$ with its estimated timing $t_j$. Calculate the cumulative distribution function $\Phi_{t_j, 0.8}(x)$ with a mean value of $t_j$ and a standard deviation of 0.8. \\
The current word $w_j$ lies in the interval with index $k = \lceil \tfrac{t_j}{l} \rceil$. For the interval $I_k$ and its neighbors $\{I_{k-2}, I_{k-1}, I_{k+1}, I_{k+2}\}$, do the cumulative mapping
\[p_{j, s} \mapsto p_{j, s} + \Phi_{t_j, 0.8}((s - 1) l) - \Phi_{t_j, 0.8}(s l),\]
for $s = k-2, k-1, \dots, k+2$. \\

Hence, we got the features and its labels for the supervised learning model described in the section below. \\
The MFCC features for an audio file are reshaped into a matrix of shape $\lbrack l, m \rbrack$, where $m$ is the number of MFCC samples for an interval of length $l$.

\section{Defining the models}

\textbf{by Paul Warkentin} \\

% source for deep convnet: https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/

For the main speech recognition part, we trained several supervised models. The following two delivered the best results.
\begin{enumerate}
	\item \textbf{Convolutional LSTM neural network} \\
	This model is a combination of a convolutional neural network and a LSTM network. The input vector will be put into a dynamic RNN cell of type \newline\textit{tf.contrib.rnn.Conv2DLSTMCell} in TensorFlow. Its output is then flattened and will be encoded by two dense layers into the label space.
	\item \textbf{Deep convolutional neural network} \\
	This model is a combination of six consecutive convolutional neural networks, each followed by activation. The output is also here flattened and will then be encoded by two dense layers into the label space. We got the idea by the blog post by Harutyunyan and Khachatrian as they did reach an accuracy of 99.24\%.
\end{enumerate}
We also implemented a model leaning on the implementation of TensorFlows' convolutional neural network for speech recognition in the example \textit{speech\_commands}\footnote{\url{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech\_commands/models.py}}. Unfortunately, this network did not deliver any usable results.
