\chapter{Introduction to the task}

There is a constant need to generate subtitles for video. For large video quantities such as on sites like youtube, machine learning systems have been employed to perform this task for some time now. However, solving the general speech-to-text problem is challenging and hence, human transcription is still used in context where subtitles need to be of consistent quality.

In this work, we aim to solve a problem which is simpler than the general speech-to-text problem: given an audio file $A$ and a finished transcript $\mathcal{T}$, we try to compute time alignment information. Let $\mathcal{T}$ be an $l$-tuple of words $w$ and $A$ be a spoken-word audio signal with known length such that every word $w_i$ in $\mathcal{T}$ occurs in $A$ at time $t_i$ . We try to find the mapping
\begin{align}
	f&\colon\textrm{Transcripts}_l\to\R^l\\
	\mathcal{T}&\mapsto f(\mathcal{T})\textrm{ such that }(f(\mathcal{T}))_i=t_i
\end{align}