\chapter{Training procedure}

\textbf{by Bastian Boll} \\

The central problem of training our models lies in the structure of the labels. For each interval, we try to predict a vector in $(0,1)^{1500}$. Each component of the vector is the probability that a specific word is contained in the interval. Naturally, most entries in the vector labels are 0 because most words are not contained in any single given interval. Trying to predict the exact label vector is not a good approach, because for a good acoustic model, many words are similar. Suppose, a given interval contains the single word \emph{we}. A good acoustic model should predict \emph{we} with high probability, but it should also give nonzero probability to \emph{she}, \emph{he}, \emph{me} or other similar sounding words. If we choose a loss function which penalizes these outputs, such as a simple mean squared error loss, it will not work well in practice. In fact, because of this effect, our models resorted to predict near zero probability for every word. We can of course mitigate this problem by interpreting the model as a single- or multiclass classifier for the few words with nonzero probability and use standard loss functions (softmax cross entropy and sigmoid cross entropy respectively). However, there is still potentially some merit in constructing a custom loss function for this specific usecase of predicting multiple classes with given label probability.

\section{Finding a good loss function}

\textbf{by Bastian Boll} \\

As outlined previously, we have multiple possible loss functions to choose from, each corresponding to a particular interpretation of the problem.

\paragraph{Mean squared error} is a very bad idea in practice. The resulting models extract the wrong information from the data: instead of finding the few nonzero probabilities, they set every predicted probability close to zero. Upon closer examination, while all predicted probabilities are close to zero (between 0.001 and 0.003), we can still see that prior proabilities for each word were also somewhat extracted from the data as very frequent words like \emph{the} consistently get higher probabilities assigned to them.

\paragraph{Softmax cross entropy} is the standard loss function for single class classification. In order to apply this to our problem, we need to label each interval with exactly one word. This presents some compromise for two reasons. Because the dataset is not precise, we may actually be providing wrong labels to model during training. This should not be a large problem because the intervals are large enough that this problem is not expected to occur very frequently. The second problem lies in the fact, that we need to throw away available information because one interval can easily contain multiple words.

\paragraph{Sigmoid cross entropy} is the standard loss function for multi class classification. By using multiple labels, we are presented with similar compromises. We now label each interval with multiple words, if the respective word probability is above a given threshold. This mitigates the first problem of losing label information in single class classification.

\paragraph{Custom loss function construction} presents the possibility to try and address both problems with the classification approaches. The main idea of our custom construction is to prioritize the model giving a high probability prediction for the word with the largest label probability in the interval. We measure this as a squared distance between the maximum label probability and the respective predicted probability. We want to also penalize a high average predicted probability. In pratice, the latter amounts to a regularization term which is scaled by a hand optimized factor between $0.3$ and $0.5$. 
This still allows for multiple guesses as to which word is most likely, but it incentivises fewer large probabilities.

\chapter{Results}
\label{results}

\textbf{by Paul Warkentin} \\

We use tensorflow for GPU accellerated training of our models. Final loss values for every combination of model an loss function are summarized in the following table.
\FloatBarrier
\begin{table}[ht]
	\label{tab:training_results}
	\centering
	\begin{tabular}{ccc}
		\toprule
		Model & Loss function & Final loss\\
		\midrule
		conv\_lstm  & softmax & 6.6850 \\
		conv\_lstm  & custom  & 0.0218 \\
		conv\_lstm  & sigmoid & 0.6922 \\
		deep\_conv  & softmax & 6.7470 \\
		deep\_conv  & custom  & 0.0227 \\
		deep\_conv  & sigmoid & 0.6931 \\
		dense\_conv & softmax & 6.7700 \\
		dense\_conv & custom  & 0.0310 \\
		dense\_conv & sigmoid & 0.7000 \\
		\bottomrule
	\end{tabular}
	\caption{Final loss values obtained in training.}
\end{table}
\FloatBarrier

In order to quantify the prediction result accuracy, we perform the optimization procedure for a talk that was not part of the training set with the predicted probabilities and measure the sum of squared differences between the optimized time points for each word and the respective time points computed from the real subtitle. Doing this for all models and loss functions as well as for optimal labels reveals multiple key insights.

\begin{enumerate}
	\item The optimization process works well for computing time alignment with optimal labels. This is further validated by listening to a demonstration in which the audio is played with each word superimposed at the computed time.
	\item All models helped to increase the accuracy of the alignment from the baseline initial guess (linear interpolation over the whole talk with subtracted silence).
	\item The required optimization time is on the order of minutes and is very inconsistent between different model predictions.
\end{enumerate}

We can also observe, that some predictions lead the optimizer to reach local minima which are not the optimal time points. By construction, this should not occur for very good predictions, but an improvement is still achieved.\\
Listening to the audio, we can see that the initial guess is already close to being sufficient for subtitle generation. The optimizer has a very slow asymptotic conververgence towards the real time points, even for perfect labels.

The following table outlines the results for optimization.

\FloatBarrier
\begin{table}[ht]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		Model & Loss function & Optimizer Steps & Time (sec) & Improvement (\%)\\
		\midrule
		optimal labels &  & 262 & 144.87 & 76.11 \\
		conv\_lstm  & softmax  & 391 & 186.98 & 2.17 \\
		conv\_lstm  & custom   & 105 & 61.56 & 2.73 \\
		conv\_lstm  & sigmoid  & 144 & 83.32 & 6.07 \\
		deep\_conv  & softmax  & 807 & 464.8 & 5.66 \\
		deep\_conv  & custom   & 286 & 167.81 & 7.44 \\
		deep\_conv  & sigmoid  & 19 & 8.42 & 8.92 \\
		dense\_conv & softmax  & 903 & 525.08 & 8.62 \\
		dense\_conv & custom   & 130 & 76.86 & 10.81 \\
		dense\_conv & sigmoid  & 955 & 514.32 & 11.59 \\
		\bottomrule
	\end{tabular}
	\caption{Percentage improvement over initial guess as measured by sum of squared differences to optimal time points.}
\end{table}
\FloatBarrier









